# GPT-2 리뷰

### [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

## 0. Abstract

Previous Problem : NLP 수행을 할때 task specific dataset에 supervised learning을 한다. → labeled training data가 필요. (많은 시간,인력이 필요함) 

Solution : GPT-2 (explicit supervision 없이 LM만을 사용하여 NLP 수행) 

- largest model with 1.5 B (15억개!!) parameters → BERT large에 비해 무려 4.5 배 더 큰 모델이다.
- WebText라는 이 논문에서 만든 데이터 세트로 학습한다
- 7 out of 8 NLP tasks에 대해 zero-shot setting으로 State of the Art 달성.

→ 제로샷러닝(zero shot learning), 원샷러닝(one shot learning), 퓨샷러닝(few shot learning) 모두 적은 추가 학습 데이터만으로 모델을 재구성하는 학습 방식. 

예를 들면, 제로샷 러닝은 **이미 사전훈련된 모델을 해당 분야용으로 추가 학습을 시키지 않고 곧바로 적용하는 훈련 방식**

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled.png)

## 1. Introduction

기존에 ML 모델들은 supervised learning으로 데이터 분포에 민감함. 심지어 레이블링이 필요해서 트레이닝 데이터를 만드는데 많은 돈과 시간이 걸린다. 

→ 그리고 큰 문제! 잘 학습된 모델도 결국에는 Narrow expert → 새로운 데이터에 대해서는 제대로 predict 하지 못함. 결국 다양한 작업에서 기존의 트레이닝 데이터를 모방하는 방식의 문제. 

Multitask learning, 즉 일반화를 잘하는 프레임워크. 이전에는 task 마다 다른 방식의 학습이 요구되었지만 self-attention을 통해 다른 task에도 똑같이 적용될 수 있다. 

BERT 모델과 같이 supervised model 은 pre-training + fine-tuning을 통해 여러 언어 task에 대해 SOTA를 달성했다. 하지만 이들의 문제점은  여전히 supervised learning을 요구한다는 것이다. 

그렇기 때문에, supervised data가 없거나 매우 적을때 사용 될 수 있는 방법으로 LM을 특정한 down-stream task를 위해 동작하게 만드는 방법을 제시한다. 

## 2. Approach

핵심은 바로? **LM (Language Model)** 

이는 보통 조건부 확률(conditional probabilities) 로 sequential하게 단어를 예측한다. 

x1,x2,..,xn : a set of examples

s1,s2,...,sn : symbols

이에 대해 general system은 많은 테스크를 수행할 수 있어야 하는데 p(output | input ) 밖에 수행을 하지 못함. 따라서 p(output | input, task), 즉 어떤 task인지 모델의 조건으로 넣어줘야한다. 

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%201.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%201.png)

이미 이 방법을 적용한 McCann 논문이 제시한 MQAN 모델 같은 경우 

(translate to french, english text, french text) : Translation Training

(answer the question, document, question, answer) : Reading Comprehension training 

이와 같은 방식으로 처음에 무슨 task를 하는 지를 언급해주고 그 다음 input들이 들어가서 출력으로 원하는 결과 (french text or answer)을 얻을 수 있다는걸 보여주었다.

→ 즉, single model for multiple different task is POSSIBLE!

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%202.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%202.png)

하지만 McCann은 실제 여러 개의 dataset을 사용하여 학습하는 거여서 unsupervised learning이 아님. 비록 fine-tuning 없이도 어떤 task에 적용 될 수 있다는 것이 novelty를 가지지만 실제로 MTL 에 비해 학습이 느리고 underfitting이라고 한다. 

다른 방법으론, 대화 시스템을 통해 general system을 만들까 했지만 이는 너무 제한적이라 적절하지 않다고 함. 

결론적으로 general system을 가진 모델을 통해 unsupervised multi-task learning을 하는게 이 논문의 목표이다. 

## 2.1 Training Dataset

이전에는 언어 모델을 트레이닝할때 News article, Wikipedia, fiction books 처럼 single domain of text를 사용했다. 

general 한 system을 만들기 위해서는 크고 다양한 도메인의 텍스트가 필요하다. 

[Common Crawl](https://commoncrawl.org/big-picture/what-we-do/) 을 사용해 많은 양의  웹의 텍스트 데이터를 얻을 수 있으나 내용이 대부분 엉망임.   따라서 새로운 Web Scrape을 사용함. 

1. 데이터의 퀄리티를 위해 첫 시작 포인트로 Reddit을 사용함. 

 2. Reddit의 글중 Karma 3개(좋아요 같은 개념) 이상을 받은 것을 사용. 따라서 사람들이 흥미롭게 느끼거나 재미를 느끼거나 도움이 됐던 text만 선정 

3. 이러한 글은 총 4천5백만개의 link가 있었음. Dragnet 과 Newpaper content extractor를 통해 각 link의 text만 추출.

4. 2017년 12월 전 post 만 추출하였고 총 40GB의 text (8백만 문서) 가 생성 되었음

5. Wikipedia 문서는 다 제거함.너무  일반적인 데이터여서 training data와 test evaluation task 와 혼동을 줄 수 있기 때문에. 

## 2.2 Input Representation

일반화된 언어모델은 어떠한 string에서라도 확률을 계산하거나 생성할 수 있어야함. 하지만 현재 large scale 언어모델은 lower-casing, tokenization, out of vocabulary tokens 같은 pre-processing 과정 때문에 모델 가능한 string들이 제한적임.

Input으로는 BPE(Byte Pair Encoding)을 사용하며 이는 character, word-level의 중간 지점이다. 

 GPT2가 사용하는 BPE는 다음과 같다

- multi-symbol tokens를 추가하기 전에 Unicode symbol을 사용하면 130,000 의 base vocabulary

(기존의 BPE는 32,000 ~ 64,000 토큰을 사용함. byte-level 버전은 256 개의 기본 어휘만 사용한다. 그렇기 때문에 이를 비교하면 GP2의 BPE는 엄청나게 많은 기본 어휘를 사용하는것.)

- dog. dog! dog? 와 같은 변형으로 일반적인 단어들의 많은 버전이 있다는걸 관찰함. →  BPE 기본 원리는 가장 많이 등장한 문자열에 대해 병합(merge)하는 작업을 원하는 단어 갯수가 될 때까지 반복한다. 이 단계에서 character category가 sequence byte 결합되는 것 방지하였음.

따라서..! BPE 방법으로 byte-level 언어모델의 일반성과 word level 언어모델의 empricial beneftis 를 모두 가져올 수 있었다.

## 2.3 Model

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%203.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%203.png)

기존 GPT 모델의 큰 구조에서 약간 변경된 모델  

아래를 제외하면 ..

- Layer Normalization이 각 sub block의 input으로 다 옮겨짐. 아마 sub block은 transformer block인 듯 하다.
- final attention block 이후 layer normalization 추가
- modified initialization  → 모델 깊이가 있는 residual layers 축적을 설명한다.
- 초기화  시 아래와 같은 비율로 조정된다. 여기서 N은 residual layer

$$1/\sqrt{N}$$

- 50,257 개의 vocabulary
- 512→ 1024 tokens로 context size 가 증가
- 총 4개의 모델을 학습함.

## GPT의 기존 모델 구조

([https://www.quantumdl.com/entry/12주차1-Improving-Language-Understanding-by-Generative-Pre-Training](https://www.quantumdl.com/entry/12%EC%A3%BC%EC%B0%A81-Improving-Language-Understanding-by-Generative-Pre-Training))

1. Unsupervised learning ( 큰 말뭉치에서 대용량의 언어모델을 학습)

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%204.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%204.png)

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%205.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%205.png)

다음과 같은 목적함수를 사용해 학습시킴. 이때 k는 context window아며 학습할 때 필요한 문맥 고려범위

- 일단 문장단위로 encoding을 한다. tokenized 된 문장을 token embedding matrix로 만드는 과정에서 Transformer 의 decoder만(12개 multi-head) 을 사용한다.  이를 거쳐 context-level embedding을 할 수 있다.  → 기존에 transformer는 6개의 decoder, 6개의 encoder로 구성되있음

2. Supervised Learning (분류 데이터를 써서 특정 과제에 맞춰 모델을 미세조정)

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%206.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%206.png)

이와 같은 목적 함수를 가짐. y는 x1~xm token을 입력으로 받았을 때의 정답 값이다. 

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%207.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%207.png)

supervised learning은 크게 2가지 부분으로 나뉨. 

1. Pretrained Model

    Text/Position Embedding부터 12개의 Decoder가 있는 부분이며 각 Embed는 BytePair Encoding으로 구성됨. Global한 NLP feature를 학습하도록 구성되어 있으며 이를 Decoder를 통해 Task specific한 feature를 추출한다. 

2.  Task Prediction/Classification

    하나의 예상만 출력하지 않음. Auxiliary Task를 통해 하나의 task 에 대해 학습 시키지 않고 sub task를 같이 학습시켜 정확도를 높임 

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%208.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%208.png)

## 3. Experiments

[GPT2 Experiments 정리](https://www.notion.so/GPT2-Experiments-59a05cf381af4e12b69674425b5234bd)

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%209.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%209.png)

## 4. Generalization vs Memorization

- 기존 컴퓨터 비전 데이터셋 같은 경우 문제가 있었음. CIFAR-10에는 train과 test간의 이미지에 3.3% overlap이 되는 것을 발견했다고 함.  그래서 자칫하면, generalization performance가 over-reporting, 즉 과하게 평가될 수 있다고 함 → **이는 워낙 크기가 큰 WebText를 만들때도 일어날 수 있음!**
- 이와 같은 문제점을 해결하기 위해서 겹치는 데이터를 확인해야 했음
    - Bloom filter(집합내에 특정 원소가 존재하는지 확인하는데 사용되는 자료구조)을 사용하였음
    - 8-gram 겹치는 정도를 데이터 간에 비교를 해서 측정해봄.
    - 많은 데이터에 overlap 문제점이 있다.
    - CoQA와 같은 경우는, document(pargraph)는 15%가 겹치지만, QA는 겹치는 게 없다.
- 이러한 overlap, similar text 가 학습에 끼치는 영향을 알아내는 것은 중요하며 현재는 n-gram 방법을 통해 중복 제거를 하는걸 추천함.

![GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%2010.png](GPT-2%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20f6b86b9b4689449783d03e6d43d6b4ee/Untitled%2010.png)

이와 같이 LM 파라미터가 늘어날 수록, train과 test 모두 perplexity(헷갈리는 정도. 낮을수록 좋음)가 떨어진다.

→  즉 GPT-2조차 아직 underfitting이 됨을 알 수가 있다.

-(high bias model, 일부 특성만 반영하여 잘못 예측)

## 6,7. Discussion  & Conclusion

- Unsupervised task learning 영역도 앞으로  더 연구되어야 한다!
- Supervision 없이 task를 배우는 pre-training 기술도 가능성 있다.
- 하지만, 실제로 사용되기엔 아직 GPT-2는 갈 길이 멀었다. evaluation task는 훨씬 더 많으며 GPT2 보다  좋은 모델들은 존재함.
- 따라서, 현재 상태는 충분한 capacity가 있을 때, 몇 개의 baseline보다 좋음 → Potential
- GPT-1 처럼 GPT-2를  GLUE, decaNLP 등에서 fine-tunning할 계획
- 하지만 BERT의 unidirectional representation은 비효율적인데 GPT-2에서 이런 것을 극복할 방법에 대해서는 불분명하다.

→ GPT-2는 transformer의 decoder을 쓰기 때문에, LM의 특성상 sequential한 상황으로 쓰는 거 같다. 즉 이렇게 하면 uni-direction이 되니까 BERT처럼 bi-direction embedding이 안되니까 말하는 것 같다.